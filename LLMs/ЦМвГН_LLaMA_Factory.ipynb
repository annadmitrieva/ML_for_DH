{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f62a6c2-53fd-422c-9f61-7796f62cc7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 338, done.\u001b[K\n",
            "remote: Counting objects: 100% (338/338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (269/269), done.\u001b[K\n",
            "remote: Total 338 (delta 74), reused 206 (delta 54), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (338/338), 9.59 MiB | 12.04 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (4.48.3)\n",
            "Collecting datasets<=3.2.0,>=2.16.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting accelerate<=1.2.1,>=0.34.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (0.21.0)\n",
            "Collecting gradio<=5.18.0,>=4.38.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading gradio-5.18.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.2.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (4.25.6)\n",
            "Collecting uvicorn (from llamafactory==0.9.2.dev0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (2.10.6)\n",
            "Collecting fastapi (from llamafactory==0.9.2.dev0)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.2.dev0)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.2.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
            "Collecting av (from llamafactory==0.9.2.dev0)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (0.10.2.post1)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "\u001b[33mWARNING: llamafactory 0.9.2.dev0 does not provide the extra 'unsloth'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.2.dev0) (2.5.1+cu124)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.2.dev0)\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.13)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (11.1.0)\n",
            "Collecting pydub (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (14.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.2.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.1.0)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.2.dev0) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.2.dev0) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.2.dev0) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
            "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.18.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25797 sha256=36dae6640c55e657b65f40c4565acf73f04ea3b8bc0a02a9ad6bd2762b6e7431\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bmlixhbb/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=a368e25fa2b775fbec8b9c1b29ba3f906a8fe3fd7a0e01caaf1b702da153d395\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, fsspec, fire, ffmpy, dill, av, anyio, aiofiles, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, datasets, bitsandbytes, accelerate, trl, peft, llamafactory\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.3.0\n",
            "    Uninstalling accelerate-1.3.0:\n",
            "      Successfully uninstalled accelerate-1.3.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.8.0 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.2.1 aiofiles-23.2.1 anyio-4.8.0 av-14.2.0 bitsandbytes-0.45.3 datasets-3.2.0 dill-0.3.8 fastapi-0.115.11 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.9.0 gradio-5.18.0 gradio-client-1.7.2 llamafactory-0.9.2.dev0 markupsafe-2.1.5 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.12.0 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.0 tiktoken-0.9.0 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes,unsloth]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "R3bejYqlPAjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ML_training_data/zlatoust_sentence_aligned_with_CATS.csv')"
      ],
      "metadata": {
        "id": "mtjK55z4PBUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[df['cos_sim'] >= 0.33]"
      ],
      "metadata": {
        "id": "5ct4BaIJR-nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['instruction'] = \"Напиши упрощенную версию текста на русском языке.\""
      ],
      "metadata": {
        "id": "qhoWLczLPDc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, extra = train_test_split(df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(extra, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "jhD4gxnoPJS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_llamafactory_json(df):\n",
        "  dicts = []\n",
        "  for n, row in df.iterrows():\n",
        "    d = {}\n",
        "    d[\"instruction\"] = row['instruction']\n",
        "    d[\"input\"] = row['source']\n",
        "    d[\"output\"] = row['target']\n",
        "    dicts.append(d)\n",
        "  return dicts"
      ],
      "metadata": {
        "id": "lTxEIboJPWlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = df_to_llamafactory_json(train_df)\n",
        "test_data = df_to_llamafactory_json(val_df)\n",
        "val_data = df_to_llamafactory_json(test_df)"
      ],
      "metadata": {
        "id": "WF0SSRp1PKDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DafBQiH4P8Bf",
        "outputId": "081e5fef-1010-4261-d54a-2c5079de8bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'instruction': 'Напиши упрощенную версию текста на русском языке.',\n",
              "  'input': 'Фандорину только двадцать один? ',\n",
              "  'output': 'Ему всего двадцать один'},\n",
              " {'instruction': 'Напиши упрощенную версию текста на русском языке.',\n",
              "  'input': 'Маша встает, берет подушку и уходит, сердитая.',\n",
              "  'output': 'Маша встаёт, берёт подушку и уходит, сердитая.'},\n",
              " {'instruction': 'Напиши упрощенную версию текста на русском языке.',\n",
              "  'input': '— спросил он',\n",
              "  'output': '— спросил Сергей. — Почему кровь? '},\n",
              " {'instruction': 'Напиши упрощенную версию текста на русском языке.',\n",
              "  'input': 'В таких мыслях она провела без него пять дней, те самые, которые он должен был находиться в отсутствии.',\n",
              "  'output': 'В таких мыслях она провела без него пять дней. '},\n",
              " {'instruction': 'Напиши упрощенную версию текста на русском языке.',\n",
              "  'input': 'Очень, очень оригинально, но… меня, собственно, не эта часть вашей статейки заинтересовала, а некоторая мысль, пропущенная в конце статьи, но которую вы, к сожалению, проводите только намеком, неясно… Одним словом, если припомните, проводится некоторый намек на то, что существуют на свете будто бы некоторые такие лица, которые могут… то есть не то что могут, а полное право имеют совершать всякие бесчинства и преступления, и что для них будто бы и закон не писан.',\n",
              "  'output': 'Одним словом, если припомните, существуют на свете будто бы некоторые такие лица, которые могут... то есть не то, что могут, а полное право имеют совершать всякие преступления, и что для них будто бы и закон не писан'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/LLaMA-Factory/data/zlatoust_train.json', 'w') as f:\n",
        "  json.dump(train_data, f, ensure_ascii = False)\n",
        "\n",
        "with open('/content/LLaMA-Factory/data/zlatoust_val.json', 'w') as f:\n",
        "  json.dump(val_data, f, ensure_ascii = False)\n",
        "\n",
        "with open('/content/LLaMA-Factory/data/zlatoust_test.json', 'w') as f:\n",
        "  json.dump(test_data, f, ensure_ascii = False)"
      ],
      "metadata": {
        "id": "SMqeg-QjSIlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/LLaMA-Factory/data/dataset_info.json', 'r') as f:\n",
        "  dataset_info = json.load(f)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_info[\"zlatoust_train\"] = {\"file_name\": \"zlatoust_train.json\"}\n",
        "dataset_info[\"zlatoust_val\"] = {\"file_name\": \"zlatoust_val.json\"}\n",
        "dataset_info[\"zlatoust_test\"] = {\"file_name\": \"zlatoust_test.json\"}"
      ],
      "metadata": {
        "id": "GUW9k8iRSm4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/LLaMA-Factory/data/dataset_info.json', 'w') as f:\n",
        "  json.dump(dataset_info, f)"
      ],
      "metadata": {
        "id": "gee_g36_S9Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7db7fe-c1d9-4cb0-bae0-adf8201ce5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-03-03 17:42:23.526586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741023743.759709    2020 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741023743.820355    2020 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-03 17:42:24.378588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://2838d4ff7401a0d01d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "2025-03-03 17:53:29.725808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741024409.747022    4819 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741024409.753367    4819 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-03-03 17:53:37] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "config.json: 100% 659/659 [00:00<00:00, 4.48MB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:53:37,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:53:37,310 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 30.8MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 7.22MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 5.69MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 9.47MB/s]\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,852 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:40,853 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-03 17:53:41,245 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:53:41,716 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:53:41,716 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:53:41,809 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-03 17:53:42,184 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-03-03 17:53:42] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-03-03 17:53:42] llamafactory.data.loader:157 >> Loading dataset zlatoust_train.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 13628 examples [00:00, 71437.38 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 13628/13628 [00:01<00:00, 6906.99 examples/s]\n",
            "[INFO|2025-03-03 17:53:45] llamafactory.data.loader:157 >> Loading dataset zlatoust_val.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 2921 examples [00:00, 57538.79 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 2921/2921 [00:00<00:00, 3056.31 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 16549/16549 [00:19<00:00, 828.76 examples/s] \n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 20195, 19077, 1802, 29380, 13932, 124436, 91406, 138811, 63262, 124662, 11916, 70895, 1478, 13073, 18108, 43055, 129743, 45310, 125670, 52571, 624, 2293, 4102, 33995, 4552, 11, 27499, 3780, 32202, 4793, 11, 18658, 5805, 9062, 13039, 1802, 142486, 1802, 1940, 128557, 7665, 91107, 3780, 1940, 1959, 4102, 33995, 15869, 36305, 143847, 126587, 126017, 88238, 143414, 18943, 126064, 142930, 4552, 11, 44358, 93078, 16104, 42796, 129751, 4552, 6020, 30, 151645, 198, 151644, 77091, 198, 2293, 128544, 71564, 17316, 19355, 130272, 11916, 11, 44358, 93078, 16104, 42796, 129751, 133484, 11, 1959, 92647, 63192, 27499, 3780, 32202, 4793, 13, 1959, 128462, 143847, 126587, 126017, 88238, 143414, 18943, 126064, 142930, 4552, 11, 44358, 93078, 16104, 42796, 129751, 133484, 30, 220, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Напиши упрощенную версию текста на русском языке.\n",
            "— Ты, Суслик, не вороти рожи… Я к слову… — Так прикажете принести графин марсалы, Максим Иваныч?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "— Не оконфузю, Максим Иванович, — ответил Суслик. — Прикажете принести графин марсалы, Максим Иванович? <|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2293, 128544, 71564, 17316, 19355, 130272, 11916, 11, 44358, 93078, 16104, 42796, 129751, 133484, 11, 1959, 92647, 63192, 27499, 3780, 32202, 4793, 13, 1959, 128462, 143847, 126587, 126017, 88238, 143414, 18943, 126064, 142930, 4552, 11, 44358, 93078, 16104, 42796, 129751, 133484, 30, 220, 151645, 198]\n",
            "labels:\n",
            "— Не оконфузю, Максим Иванович, — ответил Суслик. — Прикажете принести графин марсалы, Максим Иванович? <|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:54:08,397 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:54:08,399 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 988M/988M [00:04<00:00, 233MB/s]\n",
            "[INFO|modeling_utils.py:3904] 2025-03-03 17:54:13,146 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
            "[INFO|modeling_utils.py:1582] 2025-03-03 17:54:13,176 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1140] 2025-03-03 17:54:13,180 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4888] 2025-03-03 17:54:14,247 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4896] 2025-03-03 17:54:14,247 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.63MB/s]\n",
            "[INFO|configuration_utils.py:1095] 2025-03-03 17:54:14,485 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-03 17:54:14,489 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,k_proj,gate_proj,up_proj,down_proj,o_proj,v_proj\n",
            "[INFO|2025-03-03 17:54:14] llamafactory.model.loader:157 >> trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:741] 2025-03-03 17:54:14,998 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2369] 2025-03-03 17:54:15,550 >> ***** Running training *****\n",
            "[INFO|trainer.py:2370] 2025-03-03 17:54:15,550 >>   Num examples = 16,549\n",
            "[INFO|trainer.py:2371] 2025-03-03 17:54:15,550 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2372] 2025-03-03 17:54:15,550 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2375] 2025-03-03 17:54:15,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2376] 2025-03-03 17:54:15,551 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2377] 2025-03-03 17:54:15,551 >>   Total optimization steps = 3,102\n",
            "[INFO|trainer.py:2378] 2025-03-03 17:54:15,556 >>   Number of trainable parameters = 4,399,104\n",
            "  0% 5/3102 [00:22<3:35:43,  4.18s/it][INFO|2025-03-03 17:54:37] llamafactory.train.callbacks:157 >> {'loss': 1.3203, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 461.43}\n",
            "{'loss': 1.3203, 'grad_norm': 2.9389309883117676, 'learning_rate': 4.999967947253257e-05, 'epoch': 0.0, 'num_input_tokens_seen': 10336}\n",
            "  0% 10/3102 [00:41<3:16:59,  3.82s/it][INFO|2025-03-03 17:54:56] llamafactory.train.callbacks:157 >> {'loss': 1.1218, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 462.51}\n",
            "{'loss': 1.1218, 'grad_norm': 3.177298069000244, 'learning_rate': 4.999871789834929e-05, 'epoch': 0.01, 'num_input_tokens_seen': 19056}\n",
            "  0% 12/3102 [00:48<3:17:29,  3.83s/it][INFO|trainer.py:2643] 2025-03-03 17:55:07,323 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 51.7645, 'train_samples_per_second': 959.093, 'train_steps_per_second': 59.925, 'train_loss': 1.2623911499977112, 'epoch': 0.01, 'num_input_tokens_seen': 24096}\n",
            "  0% 12/3102 [00:51<3:42:07,  4.31s/it]\n",
            "[INFO|trainer.py:3910] 2025-03-03 17:55:07,325 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/lora/train_2025-03-03-17-42-54\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:55:07,533 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:55:07,534 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2491] 2025-03-03 17:55:07,617 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-03-03-17-42-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-03-03 17:55:07,618 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/lora/train_2025-03-03-17-42-54/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     0.0116\n",
            "  num_input_tokens_seen    =      24096\n",
            "  total_flos               =    48782GF\n",
            "  train_loss               =     1.2624\n",
            "  train_runtime            = 0:00:51.76\n",
            "  train_samples_per_second =    959.093\n",
            "  train_steps_per_second   =     59.925\n",
            "Figure saved at: saves/Qwen2.5-0.5B-Instruct/lora/train_2025-03-03-17-42-54/training_loss.png\n",
            "[WARNING|2025-03-03 17:55:08] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-03-03 17:55:08] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2025-03-03 17:55:08,151 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "config.json: 100% 1.20k/1.20k [00:00<00:00, 7.13MB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:55:49,347 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:55:49,349 >> Model config Qwen2VLConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 3.56k/3.56k [00:00<00:00, 25.0MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:01<00:00, 2.13MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 2.37MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:01<00:00, 5.33MB/s]\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,298 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:55,299 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-03 17:55:55,889 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "preprocessor_config.json: 100% 347/347 [00:00<00:00, 2.42MB/s]\n",
            "[INFO|image_processing_base.py:381] 2025-03-03 17:55:56,362 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:381] 2025-03-03 17:55:56,460 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:434] 2025-03-03 17:55:56,460 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"max_pixels\": 12845056,\n",
            "    \"min_pixels\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,549 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,549 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,549 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,550 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,550 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,550 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-03-03 17:55:56,550 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2304] 2025-03-03 17:55:57,138 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "chat_template.json: 100% 418/418 [00:00<00:00, 2.53MB/s]\n",
            "[INFO|processing_utils.py:780] 2025-03-03 17:55:58,258 >> Processor Qwen2VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"max_pixels\": 12845056,\n",
            "    \"min_pixels\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2VLProcessor\"\n",
            "}\n",
            "\n",
            "[INFO|2025-03-03 17:55:58] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:696] 2025-03-03 17:55:58,428 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-03-03 17:55:58,430 >> Model config Qwen2VLConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-VL-2B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"hidden_size\": 1536,\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-03-03 17:55:58] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "model.safetensors.index.json: 100% 56.4k/56.4k [00:00<00:00, 187MB/s]\n",
            "[INFO|modeling_utils.py:3904] 2025-03-03 17:55:58,845 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/3.99G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/3.99G [00:00<01:01, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 21.0M/3.99G [00:00<00:57, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/3.99G [00:00<00:56, 70.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/3.99G [00:00<00:54, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/3.99G [00:00<01:05, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 62.9M/3.99G [00:00<01:00, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 73.4M/3.99G [00:01<01:03, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/3.99G [00:01<00:59, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/3.99G [00:01<00:59, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 105M/3.99G [00:01<00:56, 68.9MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 115M/3.99G [00:01<00:59, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/3.99G [00:01<00:56, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/3.99G [00:02<00:54, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 147M/3.99G [00:02<00:52, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 157M/3.99G [00:02<00:53, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 168M/3.99G [00:02<00:53, 71.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/3.99G [00:02<01:02, 61.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 189M/3.99G [00:02<01:01, 62.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 199M/3.99G [00:02<00:57, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 210M/3.99G [00:03<01:03, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 220M/3.99G [00:03<01:02, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 231M/3.99G [00:03<01:06, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 241M/3.99G [00:03<01:10, 53.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 252M/3.99G [00:03<01:03, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 262M/3.99G [00:04<01:03, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 273M/3.99G [00:04<01:01, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 283M/3.99G [00:04<00:59, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 294M/3.99G [00:04<01:03, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 304M/3.99G [00:04<01:00, 61.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 315M/3.99G [00:04<00:57, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 325M/3.99G [00:05<00:58, 62.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 336M/3.99G [00:05<01:13, 49.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 346M/3.99G [00:05<01:20, 45.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 357M/3.99G [00:05<01:14, 48.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 367M/3.99G [00:06<01:17, 46.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 377M/3.99G [00:06<01:17, 46.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 388M/3.99G [00:06<01:20, 44.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 398M/3.99G [00:06<01:16, 46.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 409M/3.99G [00:06<01:10, 50.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 419M/3.99G [00:07<01:04, 55.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 430M/3.99G [00:07<01:02, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 440M/3.99G [00:07<00:57, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 451M/3.99G [00:07<00:53, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 461M/3.99G [00:07<00:54, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 472M/3.99G [00:07<00:52, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 482M/3.99G [00:08<00:50, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 493M/3.99G [00:08<00:48, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 503M/3.99G [00:08<00:48, 72.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 514M/3.99G [00:08<00:56, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 524M/3.99G [00:08<00:58, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 535M/3.99G [00:08<00:53, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 545M/3.99G [00:09<00:52, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 556M/3.99G [00:09<00:54, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 566M/3.99G [00:09<00:56, 60.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 577M/3.99G [00:09<00:56, 60.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 587M/3.99G [00:09<00:52, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 598M/3.99G [00:09<00:53, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 608M/3.99G [00:10<00:56, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 619M/3.99G [00:10<00:52, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 629M/3.99G [00:10<00:50, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 640M/3.99G [00:10<00:48, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 650M/3.99G [00:10<00:51, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 661M/3.99G [00:10<00:49, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 671M/3.99G [00:11<00:52, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 682M/3.99G [00:11<00:50, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 692M/3.99G [00:11<00:50, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 703M/3.99G [00:11<00:48, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 713M/3.99G [00:11<00:50, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 724M/3.99G [00:11<00:49, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 734M/3.99G [00:11<00:47, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 744M/3.99G [00:12<00:47, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 755M/3.99G [00:12<00:46, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 765M/3.99G [00:12<00:51, 62.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 776M/3.99G [00:12<00:57, 56.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 786M/3.99G [00:12<00:56, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 797M/3.99G [00:12<00:51, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 807M/3.99G [00:13<00:53, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 818M/3.99G [00:13<00:50, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 828M/3.99G [00:13<00:48, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 839M/3.99G [00:13<00:57, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 849M/3.99G [00:13<00:55, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 860M/3.99G [00:14<00:51, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 870M/3.99G [00:14<00:50, 61.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 881M/3.99G [00:14<00:47, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 891M/3.99G [00:14<00:45, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 902M/3.99G [00:14<00:51, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 912M/3.99G [00:14<00:52, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 923M/3.99G [00:15<00:49, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 933M/3.99G [00:15<00:46, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 944M/3.99G [00:15<00:45, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 954M/3.99G [00:15<00:44, 68.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 965M/3.99G [00:15<00:44, 68.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 975M/3.99G [00:15<00:43, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 986M/3.99G [00:15<00:43, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 996M/3.99G [00:16<00:46, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.01G/3.99G [00:16<00:45, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.02G/3.99G [00:16<00:43, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.03G/3.99G [00:16<00:47, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.04G/3.99G [00:16<00:50, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.05G/3.99G [00:16<00:50, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.06G/3.99G [00:17<00:47, 62.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.07G/3.99G [00:17<00:45, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.08G/3.99G [00:17<00:45, 63.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.09G/3.99G [00:17<00:43, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.10G/3.99G [00:17<00:44, 65.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.11G/3.99G [00:17<00:42, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.12G/3.99G [00:18<00:40, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.13G/3.99G [00:18<00:40, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.14G/3.99G [00:18<00:39, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.15G/3.99G [00:18<00:39, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.16G/3.99G [00:18<00:42, 66.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.17G/3.99G [00:18<00:43, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.18G/3.99G [00:18<00:41, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.20G/3.99G [00:19<00:40, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.21G/3.99G [00:19<00:42, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.22G/3.99G [00:19<00:42, 64.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.23G/3.99G [00:19<00:41, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.24G/3.99G [00:19<00:39, 69.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.25G/3.99G [00:19<00:42, 63.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.26G/3.99G [00:20<00:56, 48.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.28G/3.99G [00:20<00:41, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.29G/3.99G [00:20<00:43, 62.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.30G/3.99G [00:20<00:42, 62.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.31G/3.99G [00:21<00:45, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.32G/3.99G [00:21<00:45, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.33G/3.99G [00:21<00:45, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.34G/3.99G [00:21<00:42, 61.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.35G/3.99G [00:21<00:45, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.36G/3.99G [00:21<00:43, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.37G/3.99G [00:22<00:41, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.38G/3.99G [00:22<00:39, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.39G/3.99G [00:22<00:38, 68.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.41G/3.99G [00:22<00:39, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.42G/3.99G [00:22<00:43, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.43G/3.99G [00:22<00:40, 62.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.44G/3.99G [00:23<00:38, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.45G/3.99G [00:23<00:37, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.46G/3.99G [00:23<00:38, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.47G/3.99G [00:23<00:37, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.48G/3.99G [00:23<00:36, 68.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.49G/3.99G [00:23<00:37, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.50G/3.99G [00:23<00:35, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.51G/3.99G [00:24<00:34, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.52G/3.99G [00:24<00:33, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.53G/3.99G [00:24<00:33, 73.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.54G/3.99G [00:24<00:32, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.55G/3.99G [00:24<00:36, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.56G/3.99G [00:24<00:36, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.57G/3.99G [00:24<00:36, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.58G/3.99G [00:25<00:37, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.59G/3.99G [00:25<00:35, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.60G/3.99G [00:25<00:35, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.61G/3.99G [00:25<00:34, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 1.63G/3.99G [00:25<00:37, 63.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 1.64G/3.99G [00:25<00:35, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 1.65G/3.99G [00:26<00:34, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.66G/3.99G [00:26<00:34, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.67G/3.99G [00:26<00:33, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.68G/3.99G [00:26<00:35, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 1.69G/3.99G [00:26<00:34, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.70G/3.99G [00:26<00:34, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.71G/3.99G [00:27<00:33, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.72G/3.99G [00:27<00:32, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 1.73G/3.99G [00:27<00:31, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.74G/3.99G [00:27<00:37, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.75G/3.99G [00:27<00:35, 62.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.76G/3.99G [00:27<00:37, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 1.77G/3.99G [00:28<00:34, 64.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.78G/3.99G [00:28<00:33, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.79G/3.99G [00:28<00:32, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.80G/3.99G [00:28<00:33, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 1.81G/3.99G [00:28<00:34, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 1.82G/3.99G [00:28<00:33, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 1.84G/3.99G [00:28<00:31, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 1.85G/3.99G [00:29<00:30, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.86G/3.99G [00:29<00:31, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.87G/3.99G [00:29<00:31, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.88G/3.99G [00:29<00:33, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 1.89G/3.99G [00:29<00:32, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.90G/3.99G [00:29<00:30, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.91G/3.99G [00:30<00:30, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.92G/3.99G [00:30<00:30, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 1.93G/3.99G [00:30<00:38, 54.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.94G/3.99G [00:30<00:40, 51.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.95G/3.99G [00:30<00:38, 53.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.96G/3.99G [00:31<00:35, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 1.97G/3.99G [00:31<00:33, 60.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.98G/3.99G [00:31<00:31, 64.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 1.99G/3.99G [00:31<00:36, 54.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.00G/3.99G [00:31<00:33, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.01G/3.99G [00:31<00:31, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.02G/3.99G [00:32<00:30, 64.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.03G/3.99G [00:32<00:29, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.04G/3.99G [00:32<00:28, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.06G/3.99G [00:32<00:28, 68.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.07G/3.99G [00:32<00:30, 62.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.08G/3.99G [00:32<00:29, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.09G/3.99G [00:32<00:27, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.10G/3.99G [00:33<00:27, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.11G/3.99G [00:33<00:28, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.12G/3.99G [00:33<00:27, 68.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.13G/3.99G [00:33<00:27, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.14G/3.99G [00:33<00:26, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.15G/3.99G [00:33<00:25, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.16G/3.99G [00:34<00:25, 72.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.17G/3.99G [00:34<00:24, 73.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.18G/3.99G [00:34<00:24, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.19G/3.99G [00:34<00:23, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.20G/3.99G [00:34<00:26, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.21G/3.99G [00:34<00:26, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.22G/3.99G [00:34<00:26, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.23G/3.99G [00:35<00:25, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.24G/3.99G [00:35<00:26, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.25G/3.99G [00:35<00:25, 69.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.26G/3.99G [00:35<00:24, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.28G/3.99G [00:35<00:24, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.29G/3.99G [00:35<00:23, 71.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.30G/3.99G [00:35<00:23, 73.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.31G/3.99G [00:36<00:22, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.32G/3.99G [00:36<00:22, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.33G/3.99G [00:36<00:21, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.34G/3.99G [00:36<00:21, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.35G/3.99G [00:36<00:23, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.36G/3.99G [00:36<00:24, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.37G/3.99G [00:37<00:26, 61.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.38G/3.99G [00:37<00:24, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.39G/3.99G [00:37<00:23, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.40G/3.99G [00:37<00:23, 68.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.41G/3.99G [00:37<00:22, 71.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.42G/3.99G [00:37<00:21, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.43G/3.99G [00:37<00:21, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 2.44G/3.99G [00:38<00:22, 68.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.45G/3.99G [00:38<00:21, 71.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.46G/3.99G [00:38<00:21, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.47G/3.99G [00:38<00:21, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 2.49G/3.99G [00:38<00:25, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 2.50G/3.99G [00:38<00:23, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 2.51G/3.99G [00:39<00:22, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 2.52G/3.99G [00:39<00:22, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 2.53G/3.99G [00:39<00:22, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.54G/3.99G [00:39<00:24, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.55G/3.99G [00:39<00:25, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.56G/3.99G [00:39<00:23, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 2.57G/3.99G [00:40<00:24, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.58G/3.99G [00:40<00:23, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.59G/3.99G [00:40<00:25, 55.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.60G/3.99G [00:40<00:27, 51.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 2.61G/3.99G [00:40<00:25, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.62G/3.99G [00:41<00:25, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.63G/3.99G [00:41<00:23, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 2.64G/3.99G [00:41<00:22, 60.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.65G/3.99G [00:41<00:22, 60.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.66G/3.99G [00:41<00:21, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.67G/3.99G [00:41<00:20, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 2.68G/3.99G [00:42<00:19, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.69G/3.99G [00:42<00:19, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.71G/3.99G [00:42<00:18, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.72G/3.99G [00:42<00:18, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 2.73G/3.99G [00:42<00:19, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.74G/3.99G [00:42<00:19, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.75G/3.99G [00:43<00:20, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.76G/3.99G [00:43<00:18, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 2.77G/3.99G [00:43<00:18, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.78G/3.99G [00:43<00:17, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.79G/3.99G [00:43<00:19, 60.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.80G/3.99G [00:43<00:19, 61.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 2.81G/3.99G [00:43<00:18, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.82G/3.99G [00:44<00:18, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.83G/3.99G [00:44<00:19, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 2.84G/3.99G [00:44<00:17, 63.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.85G/3.99G [00:44<00:18, 62.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.86G/3.99G [00:44<00:17, 66.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.87G/3.99G [00:44<00:16, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 2.88G/3.99G [00:45<00:15, 69.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.89G/3.99G [00:45<00:15, 70.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.90G/3.99G [00:45<00:14, 72.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.92G/3.99G [00:45<00:16, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 2.93G/3.99G [00:45<00:15, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.94G/3.99G [00:45<00:15, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.95G/3.99G [00:45<00:14, 70.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.96G/3.99G [00:46<00:14, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 2.97G/3.99G [00:46<00:14, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 2.98G/3.99G [00:46<00:14, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 2.99G/3.99G [00:46<00:19, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.01G/3.99G [00:46<00:13, 73.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.02G/3.99G [00:47<00:12, 76.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.03G/3.99G [00:47<00:12, 74.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.04G/3.99G [00:47<00:27, 35.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.05G/3.99G [00:48<00:22, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.07G/3.99G [00:48<00:15, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.08G/3.99G [00:48<00:14, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.09G/3.99G [00:48<00:13, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.10G/3.99G [00:48<00:13, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.11G/3.99G [00:48<00:13, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.12G/3.99G [00:48<00:12, 68.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.14G/3.99G [00:49<00:11, 71.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.15G/3.99G [00:49<00:12, 69.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.16G/3.99G [00:49<00:12, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.17G/3.99G [00:49<00:11, 69.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.18G/3.99G [00:49<00:11, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.19G/3.99G [00:49<00:11, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.20G/3.99G [00:50<00:11, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.21G/3.99G [00:50<00:11, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.22G/3.99G [00:50<00:10, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.23G/3.99G [00:50<00:10, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.24G/3.99G [00:50<00:11, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.25G/3.99G [00:50<00:10, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.26G/3.99G [00:50<00:11, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.27G/3.99G [00:51<00:12, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 3.28G/3.99G [00:51<00:10, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.29G/3.99G [00:51<00:10, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.30G/3.99G [00:51<00:09, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.31G/3.99G [00:51<00:10, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 3.32G/3.99G [00:51<00:10, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.33G/3.99G [00:52<00:09, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.34G/3.99G [00:52<00:10, 62.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.36G/3.99G [00:52<00:09, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 3.37G/3.99G [00:52<00:09, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.38G/3.99G [00:52<00:09, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.39G/3.99G [00:52<00:09, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.40G/3.99G [00:53<00:08, 69.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 3.41G/3.99G [00:53<00:08, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.42G/3.99G [00:53<00:07, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.43G/3.99G [00:53<00:07, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.44G/3.99G [00:53<00:07, 72.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 3.45G/3.99G [00:53<00:07, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 3.46G/3.99G [00:53<00:07, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 3.47G/3.99G [00:54<00:07, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 3.48G/3.99G [00:54<00:09, 52.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.50G/3.99G [00:54<00:07, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.51G/3.99G [00:54<00:07, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 3.52G/3.99G [00:54<00:07, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.53G/3.99G [00:55<00:06, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.54G/3.99G [00:55<00:06, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.55G/3.99G [00:55<00:06, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 3.57G/3.99G [00:55<00:06, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.58G/3.99G [00:55<00:05, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.59G/3.99G [00:55<00:05, 71.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.60G/3.99G [00:55<00:05, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 3.61G/3.99G [00:56<00:05, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.62G/3.99G [00:56<00:05, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.63G/3.99G [00:56<00:04, 72.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.64G/3.99G [00:56<00:05, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 3.65G/3.99G [00:56<00:04, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.66G/3.99G [00:56<00:04, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.67G/3.99G [00:57<00:04, 70.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 3.68G/3.99G [00:57<00:05, 52.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 3.70G/3.99G [00:57<00:04, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 3.71G/3.99G [00:57<00:04, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 3.72G/3.99G [00:57<00:03, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.73G/3.99G [00:58<00:03, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.74G/3.99G [00:58<00:03, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.75G/3.99G [00:58<00:03, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 3.76G/3.99G [00:58<00:03, 71.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.77G/3.99G [00:58<00:03, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.79G/3.99G [00:58<00:02, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.80G/3.99G [00:58<00:02, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 3.81G/3.99G [00:59<00:02, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.82G/3.99G [00:59<00:02, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.83G/3.99G [00:59<00:02, 74.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.84G/3.99G [00:59<00:02, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 3.85G/3.99G [00:59<00:01, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 3.86G/3.99G [00:59<00:01, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 3.87G/3.99G [00:59<00:01, 69.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 3.88G/3.99G [01:00<00:01, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.89G/3.99G [01:00<00:01, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.90G/3.99G [01:00<00:01, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.91G/3.99G [01:00<00:01, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 3.92G/3.99G [01:00<00:01, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 3.93G/3.99G [01:00<00:00, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 3.94G/3.99G [01:00<00:00, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 3.95G/3.99G [01:01<00:00, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 3.96G/3.99G [01:01<00:00, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 3.97G/3.99G [01:01<00:00, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 3.99G/3.99G [01:01<00:00, 64.5MB/s]\n",
            "Downloading shards:  50% 1/2 [01:01<01:01, 61.98s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/429M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 10.5M/429M [00:00<00:05, 70.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 21.0M/429M [00:00<00:05, 72.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 31.5M/429M [00:00<00:06, 65.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 41.9M/429M [00:00<00:05, 67.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 52.4M/429M [00:00<00:05, 68.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 62.9M/429M [00:00<00:04, 73.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 73.4M/429M [00:01<00:04, 72.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 83.9M/429M [00:01<00:04, 72.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 94.4M/429M [00:01<00:04, 67.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 105M/429M [00:01<00:05, 64.3MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 115M/429M [00:01<00:04, 66.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 126M/429M [00:01<00:04, 69.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 136M/429M [00:02<00:05, 58.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 147M/429M [00:02<00:04, 62.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 157M/429M [00:02<00:04, 64.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 168M/429M [00:02<00:03, 66.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 178M/429M [00:02<00:04, 61.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 189M/429M [00:02<00:03, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 199M/429M [00:02<00:03, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 210M/429M [00:03<00:03, 66.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 220M/429M [00:03<00:03, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 231M/429M [00:03<00:02, 69.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 241M/429M [00:03<00:02, 69.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 252M/429M [00:03<00:02, 70.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 262M/429M [00:03<00:02, 70.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 273M/429M [00:04<00:02, 67.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 283M/429M [00:04<00:02, 68.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 294M/429M [00:04<00:02, 66.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 304M/429M [00:04<00:01, 67.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 315M/429M [00:04<00:01, 65.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 325M/429M [00:04<00:01, 67.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 336M/429M [00:04<00:01, 69.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 346M/429M [00:05<00:01, 57.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 357M/429M [00:05<00:01, 60.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 367M/429M [00:05<00:00, 63.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 377M/429M [00:05<00:00, 65.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 388M/429M [00:05<00:00, 63.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 398M/429M [00:06<00:00, 65.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 409M/429M [00:06<00:00, 63.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 419M/429M [00:06<00:00, 66.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 429M/429M [00:06<00:00, 66.1MB/s]\n",
            "Downloading shards: 100% 2/2 [01:08<00:00, 34.29s/it]\n",
            "[INFO|modeling_utils.py:1582] 2025-03-03 17:57:07,430 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1140] 2025-03-03 17:57:07,433 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1582] 2025-03-03 17:57:07,434 >> Instantiating Qwen2VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
            "[WARNING|logging.py:328] 2025-03-03 17:57:07,580 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
            "Loading checkpoint shards: 100% 2/2 [00:19<00:00,  9.82s/it]\n",
            "[INFO|modeling_utils.py:4888] 2025-03-03 17:57:27,418 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4896] 2025-03-03 17:57:27,418 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-2B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 2.04MB/s]\n",
            "[INFO|configuration_utils.py:1095] 2025-03-03 17:57:27,646 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B/snapshots/d3a53f2484fce9d62fff115a5ddfc833f873bfde/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-03-03 17:57:27,646 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.1,\n",
            "  \"top_k\": 1,\n",
            "  \"top_p\": 0.001\n",
            "}\n",
            "\n",
            "[INFO|2025-03-03 17:57:27] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-03-03 17:57:27] llamafactory.model.loader:157 >> all params: 2,208,985,600\n",
            "[WARNING|2025-03-03 17:57:27] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2923, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 116, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 94, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2829, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2925, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://2838d4ff7401a0d01d.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",                         # use alpaca and identity datasets\n",
        "  template=\"llama3\",                                         # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                                    # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                                         # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,                             # the micro batch size\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  logging_steps=5,                                           # log every 5 steps\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  save_steps=1000,                                           # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                                        # the learning rate\n",
        "  num_train_epochs=3.0,                                      # the epochs of training\n",
        "  max_samples=500,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                                                 # use float16 mixed precision training\n",
        "  report_to=\"none\",                                          # disable wandb logging\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                         # same to the one in training\n",
        "  finetuning_type=\"lora\",                                    # same to the one in training\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                       # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                        # same to the one in training\n",
        "  finetuning_type=\"lora\",                                   # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",                          # the path to save the merged model\n",
        "  export_size=2,                                            # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                                      # the device used in export, can be chosen from `cpu` and `auto`\n",
        "  # export_hub_model_id=\"your_id/your_model\",               # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}